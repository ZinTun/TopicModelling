{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModelling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+3+KlCqyI6HqwtjXGGFgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZinTun/TopicModelling/blob/main/TopicModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLubN6PRHWI8"
      },
      "source": [
        "Twitter API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TCBu18sGBRY"
      },
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "####input your credentials here\n",
        "consumer_key = 'xxx'\n",
        "consumer_secret = 'xxx'\n",
        "access_token = 'xxx'\n",
        "access_token_secret = 'xxx'\n",
        "\n",
        "####\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "tweet_list=[]\n",
        "#### getting tweets with #Feb19Coup\n",
        "tweets = tweepy.Cursor(api.search, q=\"#Feb19Coup\", lang = \"en\", tweet_mode='extended').items()\n",
        "for tweet_info in tweets:\n",
        "  ### the full_text retrieval for retweet and normal tweets are not the same\n",
        "  if 'retweeted_status' in dir(tweet_info):\n",
        "      tweet_list.append(tweet_info.retweeted_status.full_text)\n",
        "  else:\n",
        "      tweet_list.append(tweet_info.full_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivr1xaarHb4A"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glx39mAOG3Et"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "import string\n",
        "\n",
        "### lower case \n",
        "doc_lowercase = [doc.lower() for doc in tweet_list]\n",
        "\n",
        "### remove links\n",
        "doc_nolinks = [re.sub('http\\S+', ' ', doc) for doc in doc_lowercase]\n",
        "\n",
        "## remove \\n\n",
        "doc_wolinefeed = [re.sub('\\s+', ' ', doc) for doc in doc_nolinks]\n",
        "\n",
        "## remove @\n",
        "doc_noemail = [re.sub('\\S*@\\S*\\s?', '', doc) for doc in doc_wolinefeed]\n",
        "\n",
        "## remove emoji\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "                           \n",
        "doc_noemoji = [emoji_pattern.sub(r'', doc) for doc in doc_noemail]\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "## adding additional stop-words after analyzing the original corpus\n",
        "stop_words.extend(['#whatshappeninginmyanmar', '#feb19coup', \"us\", \"u\"])\n",
        "exclude = set(string.punctuation)\n",
        "ps = PorterStemmer() \n",
        "\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop_words])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    stemmed = \" \".join( ps.stem(word) for word in punc_free.split())\n",
        "    return stemmed\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_noemoji] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJhZOU9RHeKd"
      },
      "source": [
        "LDA Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNL_pWQCHIIh"
      },
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary \n",
        "mydict = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting into Document Term Matrix \n",
        "doc_term_matrix = [mydict.doc2bow(doc,allow_update=True) for doc in doc_clean]\n",
        "print(doc_term_matrix)\n",
        "\n",
        "# Create LDA object using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# train LDA model\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = mydict, passes=50)\n",
        "\n",
        "#save model\n",
        "ldamodel.save('lda_model.model')\n",
        "\n",
        "ldamodel.print_topics(-1)\n",
        "# [(0,\n",
        "#   '0.041*\"thwate\" + 0.035*\"peac\" + 0.026*\"mya\" + 0.025*\"junta\" + 0.025*\"head\" + 0.024*\"polic\" + 0.024*\"shot\" + 0.024*\"demonstr\" + 0.023*\"kha\" + 0.022*\"bullet\"'),\n",
        "#  (1,\n",
        "#   '0.045*\"need\" + 0.042*\"justic\" + 0.034*\"protest\" + 0.030*\"myanmar\" + 0.027*\"myitkyina\" + 0.022*\"militari\" + 0.019*\"peac\" + 0.014*\"support\" + 0.014*\"whatshappeninglnmyanmar\" + 0.012*\"democraci\"'),\n",
        "#  (2,\n",
        "#   '0.034*\"myitkyina\" + 0.030*\"teacher\" + 0.030*\"arrest\" + 0.026*\"peopl\" + 0.023*\"polic\" + 0.022*\"militari\" + 0.022*\"two\" + 0.018*\"want\" + 0.017*\"need\" + 0.016*\"justic\"')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BXWCgkuHizw"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f4adQZHHN1M"
      },
      "source": [
        "topics=ldamodel.show_topics(num_topics=3, num_words=20,formatted=False)\n",
        "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in topics]\n",
        "\n",
        "def drawwordcloud(filename):\n",
        "  wordcloud = WordCloud(max_font_size=200, max_words=75, font_step=2, width= 800, height=400, background_color=\"white\").generate(text)\n",
        "  wordcloud.to_file(filename)\n",
        "\n",
        "  plt.figure(dpi=300)\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "count = 1;\n",
        "for topic,words in topics_words:\n",
        "    text = \" \".join(words)\n",
        "    drawwordcloud(\"topic\"+str(count)+\".png\")\n",
        "    count = count +1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}